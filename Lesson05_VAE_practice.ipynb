{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK4E9sbpj7XrgGTx/qFqph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lyshen/PiggyToy/blob/main/Lesson05_VAE_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 5. Simple VAE excerise\n",
        "1. Re-use lesson 3's code\n",
        "2. Try to implement your simple VAE using the MNIST dataset\n",
        "3. Explore some simple ideas about the generative model\n",
        "4. Compare the results with Pytorch\n"
      ],
      "metadata": {
        "id": "vpv5f55rfdQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transform"
      ],
      "metadata": {
        "id": "yDJZRguWf2ju"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tt = transform.Compose([transform.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root='./data', train=True, transform=tt, download=True)\n",
        "test_data = MNIST(root='./data', train=False, transform=tt)"
      ],
      "metadata": {
        "id": "3FrBz4bHXlSM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_from_torch_to_numpy(torch_data):\n",
        "    features_np = torch_data.data.numpy()\n",
        "    labels_np = torch_data.targets.numpy()\n",
        "\n",
        "    features_np = np.resize(features_np, (features_np.shape[0], features_np.shape[1] * features_np.shape[2]))\n",
        "    labels = np.zeros((labels_np.shape[0], 10))\n",
        "    for i in range(10):\n",
        "        labels[labels_np==i, i] = 1.\n",
        "\n",
        "    return features_np, labels\n",
        "\n",
        "train_features, train_labels = transform_from_torch_to_numpy(train_data)\n",
        "test_features, test_labels = transform_from_torch_to_numpy(test_data)"
      ],
      "metadata": {
        "id": "1uf8jnNwYsr6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_dataset(n, batch_size):\n",
        "    idices= np.array(range(n))\n",
        "    np.random.shuffle(idices)\n",
        "    batch_sample_size = int((n - n % batch_size) / batch_size)\n",
        "    m = int(batch_size * batch_sample_size)\n",
        "    a = idices[0:m].reshape((batch_size, batch_sample_size))\n",
        "    b = idices[m:].reshape((1, n % batch_size))\n",
        "    return a\n",
        "\n",
        "def get_precision(predict, y):\n",
        "    a = np.argmax(predict, axis=1)\n",
        "    b = np.argmax(y, axis=1)\n",
        "    print(np.sum(a == b) / y.shape[0])"
      ],
      "metadata": {
        "id": "33HkYc9UvLGO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re implement simple DNN\n",
        "sigmoid = lambda a: 1.0 / (1.0 + np.exp(-a))\n",
        "\n",
        "# array / vector  very carefully\n",
        "softmax = lambda a: np.exp(a) / (np.sum(np.exp(a), axis=1)[:, np.newaxis])\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self, in_size, out_size = 1):\n",
        "        self.in_size = in_size + 1\n",
        "        self.out_size = out_size\n",
        "        self.X = None\n",
        "        self.W = np.random.uniform(-1., 1., size=(self.in_size, self.out_size))\n",
        "        self.dL_by_dW = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
        "        Z = self.X.dot(self.W)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, dL_by_dZ):\n",
        "        m = dL_by_dZ.shape[0]\n",
        "        # Matrix Z = XW   X(m, n+1) W(n+1, out) Z(m, out)\n",
        "        # dL/dZ = (m, out) dZ/dX = W.T  (out, n+1)  dZ/dW = X.T (n+1, m)\n",
        "        # dL/dW = dZ/dW * dL/dZ  (n+1, out)\n",
        "        # dL/dX = dL/dZ * dZ/dX  (m, n+1)\n",
        "\n",
        "        # backward dL/dX and record dL/dW\n",
        "        dZ_by_dW = self.X.T  #(n+1, m)\n",
        "        dZ_by_dX = self.W.T  #(out, n+1)\n",
        "        dL_by_dX = dL_by_dZ.dot(dZ_by_dX) #(m, n+1)\n",
        "        #dZ/dW = X.T (n+1, m) .dot dL/dZ = (m, out)\n",
        "        self.dL_by_dW = dZ_by_dW.dot(dL_by_dZ) / m #(n+1, out)\n",
        "        return dL_by_dX[:, :-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.concatenate((X, np.ones((X.shape[0], 1))), axis=1)\n",
        "        Z = X.dot(self.W)\n",
        "        return Z\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        self.W = self.W - self.dL_by_dW * learning_rate\n",
        "\n",
        "\n",
        "class Sigmoid_Active_Function:\n",
        "    def __init__(self):\n",
        "        self.active_function = sigmoid\n",
        "        self.H = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.H = self.active_function(X)\n",
        "        return self.H\n",
        "\n",
        "    def backward(self, up_gradient):\n",
        "        self.local_gradient = (1 - self.H) * self.H\n",
        "        down_gradient = up_gradient * self.local_gradient\n",
        "        return down_gradient\n",
        "\n",
        "    def predict(self, X):\n",
        "        H = self.active_function(X)\n",
        "        return H\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        return\n",
        "\n",
        "class Cross_Entropy_Loss:\n",
        "    def __init__(self):\n",
        "        self.process_function = softmax\n",
        "\n",
        "    def forward(self, Z, Y):\n",
        "        #self.Y_hat = self.process_function(Z)\n",
        "        self.Y_hat = Z\n",
        "        self.loss = -1. * np.sum(Y * np.log(self.Y_hat, where= self.Y_hat != 0)) #KL divergence\n",
        "        return self.Y_hat, self.loss\n",
        "\n",
        "    def backward(self, Y):\n",
        "        # Math provement\n",
        "        self.dL_by_dZ = self.Y_hat - Y\n",
        "        return self.dL_by_dZ\n",
        "\n",
        "class Operator_Packager:\n",
        "    def __init__(self, operator_list):\n",
        "        self.operator_list = operator_list\n",
        "\n",
        "    def forward(self, X):\n",
        "        Z = X\n",
        "        size = len(self.operator_list)\n",
        "        for i in range(size):\n",
        "            Z = self.operator_list[i].forward(Z)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, E):\n",
        "        d = E\n",
        "        size = len(self.operator_list)\n",
        "        for i in range(size):\n",
        "            index = size - 1 - i\n",
        "            d = self.operator_list[index].backward(d)\n",
        "        return d\n",
        "\n",
        "    def predict(self, X):\n",
        "        Z = X\n",
        "        size = len(self.operator_list)\n",
        "        for i in range(size):\n",
        "            Z = self.operator_list[i].predict(Z)\n",
        "        Y_hat = softmax(Z)\n",
        "        return Y_hat\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        size = len(self.operator_list)\n",
        "        for i in range(size):\n",
        "            self.operator_list[i].update_weights(learning_rate)\n"
      ],
      "metadata": {
        "id": "mrG66EpVpLW9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE:\n",
        "    def __init__(self, size_of_features, size_of_bottle):\n",
        "        self.model = Operator_Packager([\n",
        "            Linear(size_of_features, 100),\n",
        "            Sigmoid_Active_Function(),\n",
        "            Linear(100, 50),\n",
        "            Sigmoid_Active_Function(),\n",
        "            Linear(50, size_of_bottle),\n",
        "            Linear(size_of_bottle, 50),\n",
        "            Sigmoid_Active_Function(),\n",
        "            Linear(50, 100),\n",
        "            Sigmoid_Active_Function(),\n",
        "            Linear(100, size_of_features)\n",
        "        ])\n",
        "\n",
        "    def forward(self, X):\n",
        "        Z = self.model.forward(X)\n",
        "        return Z\n",
        "\n",
        "    def backward(self, E):\n",
        "        d = self.model.backward(E)\n",
        "        return d\n",
        "\n",
        "    def update_weights(self, learning_rate):\n",
        "        self.model.update_weights(learning_rate)\n",
        "\n",
        "    def encode(self, X):\n",
        "        return X\n",
        "\n",
        "    def decode(self, ):\n",
        "        return X"
      ],
      "metadata": {
        "id": "Ipasw1YQs8Eu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_vae = VAE(28*28, 2)\n",
        "loss = Cross_Entropy_Loss()\n",
        "\n",
        "epochs = 200\n",
        "m = train_labels.shape[0]\n",
        "for t in range(epochs):\n",
        "    if t % 10 == 0:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "\n",
        "    batch_size = 60\n",
        "    batch_matrix = shuffle_dataset(m, batch_size)\n",
        "\n",
        "    l = 0.\n",
        "    for batch_id in range(batch_size):\n",
        "        batch_index = batch_matrix[batch_id, :]\n",
        "        X = train_features[batch_index, :]\n",
        "        Y = train_labels[batch_index, :]\n",
        "\n",
        "        Z = demo_vae.forward(X)\n",
        "        predicts, l = loss.forward(Z, Y)\n",
        "\n",
        "        dL_by_dZ = loss.backward(Y)\n",
        "        dL_by_dW = demo_vae.backward(dL_by_dZ)\n",
        "\n",
        "        demo_vae.update_weights(learning_rate = 0.5)\n",
        "\n",
        "    print('t:', t+1, ' loss :', l)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "InjWlF3Rs2s6",
        "outputId": "8b036844-7f3b-4be5-f167-c6a80f9d963b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-9b9e1908da8e>:2: RuntimeWarning: overflow encountered in exp\n",
            "  sigmoid = lambda a: 1.0 / (1.0 + np.exp(-a))\n",
            "<ipython-input-21-9b9e1908da8e>:72: RuntimeWarning: invalid value encountered in log\n",
            "  self.loss = -1. * np.sum(Y * np.log(self.Y_hat, where= self.Y_hat != 0)) #KL divergence\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0f870d98a9ca>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemo_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdL_by_dZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-9b9e1908da8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Z, Y)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#self.Y_hat = self.process_function(Z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_hat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#KL divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1000,10) (1000,784) "
          ]
        }
      ]
    }
  ]
}